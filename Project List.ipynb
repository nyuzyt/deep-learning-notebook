{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 1. Your first neural network\n",
    "## Goal\n",
    "> Build a 3-layer neural network with basic libs\n",
    "\n",
    "## Main Libs\n",
    "> numpy, pandas, matplotlib.pyplot\n",
    "\n",
    "## Technology & Core Code\n",
    "* **Dummy Variable**\n",
    "\n",
    "```python\n",
    "dummy_fields = ['season', 'weathersit', 'mnth', 'hr', 'weekday']\n",
    "for each in dummy_fields:\n",
    "    dummies = pd.get_dummies(rides[each], prefix=each, drop_first=False)\n",
    "    rides = pd.concat([rides, dummies], axis=1)\n",
    "\n",
    "fields_to_drop = ['instant', 'dteday', 'season', 'weathersit', \n",
    "                  'weekday', 'atemp', 'mnth', 'workingday', 'hr']\n",
    "data = rides.drop(fields_to_drop, axis=1)\n",
    "```\n",
    "* **Scaling Target Variables**\n",
    "\n",
    "```python\n",
    "quant_features = ['casual', 'registered', 'cnt', 'temp', 'hum', 'windspeed']\n",
    "# Store scalings in a dictionary so we can convert back later\n",
    "scaled_features = {}\n",
    "for each in quant_features:\n",
    "    mean, std = data[each].mean(), data[each].std()\n",
    "    scaled_features[each] = [mean, std]\n",
    "    data.loc[:, each] = (data[each] - mean)/std\n",
    "```\n",
    "* **Weight Initialization**\n",
    "\n",
    "```python\n",
    "self.weights_input_to_hidden = np.random.normal(0.0, self.hidden_nodes**-0.5, \n",
    "                                       (self.hidden_nodes, self.input_nodes))\n",
    "\n",
    "self.weights_hidden_to_output = np.random.normal(0.0, self.output_nodes**-0.5, \n",
    "                                       (self.output_nodes, self.hidden_nodes))\n",
    "```\n",
    "* **Forward Pass**\n",
    "\n",
    "```python\n",
    "# TODO: Hidden layer\n",
    "hidden_inputs = np.dot(self.weights_input_to_hidden, inputs)\n",
    "hidden_outputs = self.activation_function(hidden_inputs)\n",
    "        \n",
    "# TODO: Output layer\n",
    "final_inputs = np.dot(self.weights_hidden_to_output, hidden_outputs)\n",
    "final_outputs = final_inputs\n",
    "```\n",
    "* **Backpropagation**\n",
    "\n",
    "```python\n",
    "# TODO: Output error\n",
    "output_errors = targets - final_outputs\n",
    "        \n",
    "# TODO: Backpropagated error\n",
    "hidden_errors = np.dot(self.weights_hidden_to_output.T, output_errors)\n",
    "hidden_grad = hidden_outputs * (1.0 - hidden_outputs)\n",
    "\n",
    "# TODO: Update the weights\n",
    "self.weights_hidden_to_output += self.lr * np.dot(output_errors, hidden_outputs.T)\n",
    "self.weights_input_to_hidden += self.lr * np.dot(hidden_errors * hidden_grad, inputs.T)\n",
    "```\n",
    "## Path\n",
    "> deep-learning/first-neural-network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Sentiment Classification\n",
    "## Goal\n",
    "> Build a neural network which can tell positive and negative reviews\n",
    "\n",
    "## Main Libs\n",
    "> collections, numpy\n",
    "\n",
    "## Technology & Core Code\n",
    "* **Count Word**\n",
    "\n",
    "```python\n",
    "# Create three Counter objects to store positive, negative and total counts\n",
    "positive_counts = Counter()\n",
    "negative_counts = Counter()\n",
    "total_counts = Counter()\n",
    "\n",
    "# Loop over all the words in all the reviews and increment the counts in the appropriate counter objects\n",
    "for i in range(len(reviews)):\n",
    "    if(labels[i] == 'POSITIVE'):\n",
    "        for word in reviews[i].split(\" \"):\n",
    "            positive_counts[word] += 1\n",
    "            total_counts[word] += 1\n",
    "    else:\n",
    "        for word in reviews[i].split(\" \"):\n",
    "            negative_counts[word] += 1\n",
    "            total_counts[word] += 1\n",
    "```\n",
    "* **Ratio of Postive to Negative**\n",
    "\n",
    "```python\n",
    "pos_neg_ratios = Counter()\n",
    "\n",
    "# Calculate the ratios of positive and negative uses of the most common words\n",
    "# Consider words to be \"common\" if they've been used at least 100 times\n",
    "for term,cnt in list(total_counts.most_common()):\n",
    "    if(cnt > 100):\n",
    "        pos_neg_ratio = positive_counts[term] / float(negative_counts[term]+1)\n",
    "        pos_neg_ratios[term] = pos_neg_ratio\n",
    "        \n",
    "# Convert ratios to logs\n",
    "for word,ratio in pos_neg_ratios.most_common():\n",
    "    if(ratio > 1):\n",
    "        pos_neg_ratios[word] = np.log(ratio)\n",
    "    else:\n",
    "        pos_neg_ratios[word] = -np.log((1 / (ratio+0.01)))\n",
    "```\n",
    "* **Create the Input/Output**\n",
    "\n",
    "```python\n",
    "vocab = set(total_counts.keys())\n",
    "vocab_size = len(vocab)\n",
    "layer_0 = np.zeros((1,vocab_size))\n",
    "\n",
    "# Create a dictionary of words in the vocabulary mapped to index positions \n",
    "# (to be used in layer_0)\n",
    "word2index = {}\n",
    "for i,word in enumerate(vocab):\n",
    "    word2index[word] = i\n",
    "   \n",
    "def update_input_layer(review):\n",
    "    \"\"\" \n",
    "    The element at a given index of layer_0 should represent\n",
    "    how many times the given word occurs in the review.\n",
    "    Args:\n",
    "        review(string) - the string of the review\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "     \n",
    "    global layer_0\n",
    "    \n",
    "    # clear out previous state, reset the layer to be all 0s\n",
    "    layer_0 *= 0\n",
    "    \n",
    "    # count how many times each word is used in the given review and store the results in layer_0 \n",
    "    for word in review.split(\" \"):\n",
    "        layer_0[0][word2index[word]] += 1\n",
    "```\n",
    "* **Reduce Noise**\n",
    "\n",
    "```python\n",
    "def update_input_layer(self,review):\n",
    "\n",
    "        # clear out previous state, reset the layer to be all 0s\n",
    "        self.layer_0 *= 0\n",
    "        \n",
    "        for word in review.split(\" \"):\n",
    "            # NOTE: This if-check was not in the version of this method created in Project 2,\n",
    "            #       and it appears in Andrew's Project 3 solution without explanation. \n",
    "            #       It simply ensures the word is actually a key in word2index before\n",
    "            #       accessing it, which is important because accessing an invalid key\n",
    "            #       with raise an exception in Python. This allows us to ignore unknown\n",
    "            #       words encountered in new reviews.\n",
    "            if(word in self.word2index.keys()):\n",
    "                ## New for Project 4: changed to set to 1 instead of add 1\n",
    "                self.layer_0[0][self.word2index[word]] = 1\n",
    "```\n",
    "* **Reduce Inefficiencies**\n",
    "```python\n",
    "## New for Project 5: pre-process training reviews so we can deal \n",
    "        #                     directly with the indices of non-zero inputs\n",
    "        training_reviews = list()\n",
    "        for review in training_reviews_raw:\n",
    "            indices = set()\n",
    "            for word in review.split(\" \"):\n",
    "                if(word in self.word2index.keys()):\n",
    "                    indices.add(self.word2index[word])\n",
    "            training_reviews.append(list(indices))\n",
    "```\n",
    "* **Strategically Reducing the Vocabulary**\n",
    "```python\n",
    "# populate review_vocab with all of the words in the given reviews\n",
    "review_vocab = set()\n",
    "for review in reviews:\n",
    "    for word in review.split(\" \"):\n",
    "        ## New for Project 6: only add words that occur at least min_count times\n",
    "        #                     and for words with pos/neg ratios, only add words\n",
    "        #                     that meet the polarity_cutoff\n",
    "        if(total_counts[word] > min_count):\n",
    "            if(word in pos_neg_ratios.keys()):\n",
    "                if((pos_neg_ratios[word] >= polarity_cutoff) or (pos_neg_ratios[word] <= -polarity_cutoff)):\n",
    "                    review_vocab.add(word)\n",
    "                else:\n",
    "                    review_vocab.add(word)\n",
    "```\n",
    "## Path\n",
    "> deep-learning/sentiment-network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Sentiment Analysis with TFLearn\n",
    "## Goal\n",
    "> Build a neural network for sentiment analysis using a library called TFLearn\n",
    "\n",
    "## Main Libs\n",
    "> pandas, numpy, tensorflow, tflearn\n",
    "\n",
    "## Technology\n",
    "> Count Word\n",
    "\n",
    "> TFLearn functions\n",
    "\n",
    "## Technology & Core Code\n",
    "* **Train, Validation, Test sets**\n",
    "\n",
    "```python\n",
    "Y = (labels=='positive').astype(np.int_)\n",
    "records = len(labels)\n",
    "\n",
    "shuffle = np.arange(records)\n",
    "np.random.shuffle(shuffle)\n",
    "test_fraction = 0.9\n",
    "\n",
    "train_split, test_split = shuffle[:int(records*test_fraction)], shuffle[int(records*test_fraction):]\n",
    "trainX, trainY = word_vectors[train_split,:], to_categorical(Y.values[train_split], 2)\n",
    "testX, testY = word_vectors[test_split,:], to_categorical(Y.values[test_split], 2)\n",
    "```\n",
    "\n",
    "* **Building the network**\n",
    "\n",
    "```python\n",
    "# Network building\n",
    "def build_model():\n",
    "    # This resets all parameters and variables, leave this here\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    #### Your code ####\n",
    "    net = tflearn.input_data([None, 10000])                          # Input\n",
    "    net = tflearn.fully_connected(net, 200, activation='ReLU')      # Hidden\n",
    "    net = tflearn.fully_connected(net, 25, activation='ReLU')      # Hidden\n",
    "    \n",
    "    net = tflearn.fully_connected(net, 2, activation='softmax')   # Output\n",
    "    net = tflearn.regression(net, optimizer='sgd', learning_rate=0.01, loss='categorical_crossentropy')\n",
    "    \n",
    "    model = tflearn.DNN(net)\n",
    "    return model\n",
    "```\n",
    "\n",
    "## Path\n",
    "> deep-learning/intro-to-tflearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. TensorFlow Neural Network Lab\n",
    "## Goal\n",
    "> Labeling images of English letters with tensorflow\n",
    "\n",
    "## Main Libs\n",
    "> tensorflow, sklearn\n",
    "\n",
    "## Technology & Core Code\n",
    "* **normalization**\n",
    "Min-Max Scaling:\n",
    "\n",
    "$$\n",
    "X'=a+{\\frac {\\left(X-X_{\\min }\\right)\\left(b-a\\right)}{X_{\\max }-X_{\\min }}}\n",
    "$$\n",
    "* **One-Hot Encode**\n",
    "\n",
    "```python\n",
    "if not is_labels_encod:\n",
    "    # Turn labels into numbers and apply One-Hot Encoding\n",
    "    encoder = LabelBinarizer()\n",
    "    encoder.fit(train_labels)\n",
    "    train_labels = encoder.transform(train_labels)\n",
    "    test_labels = encoder.transform(test_labels)\n",
    "```\n",
    "* **split data**\n",
    "\n",
    "```python\n",
    "# Get randomized datasets for training and validation\n",
    "train_features, valid_features, train_labels, valid_labels = train_test_split(\n",
    "    train_features,\n",
    "    train_labels,\n",
    "    test_size=0.05,\n",
    "    random_state=832289)\n",
    "```\n",
    "\n",
    "* **tensorflow initialization**\n",
    "\n",
    "```python\n",
    "# TODO: Set the features and labels tensors\n",
    "features = tf.placeholder(tf.float32,[None, features_count])\n",
    "labels = tf.placeholder(tf.float32,[None, labels_count])\n",
    "\n",
    "# TODO: Set the weights and biases tensors\n",
    "weights = tf.Variable(tf.truncated_normal((features_count, labels_count)))\n",
    "biases = tf.Variable(tf.zeros(labels_count))\n",
    "```\n",
    "\n",
    "## Path\n",
    "> deep-learning/intro-to-tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Image Classification\n",
    "## Goal\n",
    "> Train a convolutional neural network to classify images\n",
    "\n",
    "## Main Libs\n",
    "> tensorflow, numpy\n",
    "\n",
    "## Technology & Core Code\n",
    "* **normalization**\n",
    "\n",
    "```python\n",
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    norm_data = np.array(x/255)\n",
    "    return norm_data\n",
    "```\n",
    "* **One-Hot encode**\n",
    "\n",
    "```python\n",
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    lb = preprocessing.LabelBinarizer()\n",
    "    \n",
    "    # The encoder finds the classes\n",
    "    lb.fit(np.array(list(range(10))))\n",
    "    \n",
    "    return lb.transform(x)\n",
    "```\n",
    "* **Convolution and Max Pooling Layer**\n",
    "\n",
    "```python\n",
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    # print(x_tensor.shape, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    \n",
    "    # print(x_tensor.get_shape().as_list()[3])\n",
    "    \n",
    "    conv_weights = tf.Variable(tf.truncated_normal((conv_ksize[0],conv_ksize[1],x_tensor.get_shape().as_list()[3],conv_num_outputs),mean=0.0, stddev=0.1))\n",
    "\n",
    "    stride = [1] + list(conv_strides) + [1]    \n",
    "    padding = 'VALID'    \n",
    "    bias = tf.Variable (tf.zeros(conv_num_outputs))\n",
    "    \n",
    "    conv = tf.nn.conv2d(x_tensor, conv_weights, stride, padding)\n",
    "    conv = tf.nn.bias_add(conv, bias)\n",
    "    conv = tf.nn.relu(conv)\n",
    "    \n",
    "    pool_ksize = [1] + list(pool_ksize) + [1]\n",
    "    pool_strides = [1] + list(pool_strides) + [1]\n",
    "    \n",
    "    return tf.nn.max_pool(conv, pool_ksize, pool_strides, padding)\n",
    "```\n",
    "\n",
    "* **Fully-Connected Layer**\n",
    "```python\n",
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.contrib.layers.fully_connected(x_tensor, num_outputs, activation_fn=tf.nn.relu)\n",
    "```\n",
    "* **Output Layer**\n",
    "```python\n",
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.contrib.layers.fully_connected(x_tensor, num_outputs, activation_fn=None)\n",
    "```\n",
    "\n",
    "## Path\n",
    "> deep-learning/image-classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Anna KaRNNa\n",
    "## Goal\n",
    "> Building a character-wise RNN trained on Anna Karenina, it'll be able to generate new text based on the text from the book\n",
    "\n",
    "## Main Libs\n",
    "> collection, numpy, tensorflow\n",
    "\n",
    "## Technology & Core Code\n",
    "* **split data**\n",
    "\n",
    "```python\n",
    "def split_data(chars, batch_size, num_steps, split_frac=0.9):\n",
    "    \"\"\" \n",
    "    Split character data into training and validation sets, inputs and targets for each set.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    chars: character array\n",
    "    batch_size: Size of examples in each of batch\n",
    "    num_steps: Number of sequence steps to keep in the input and pass to the network\n",
    "    split_frac: Fraction of batches to keep in the training set\n",
    "    \n",
    "    \n",
    "    Returns train_x, train_y, val_x, val_y\n",
    "    \"\"\"\n",
    "    \n",
    "    slice_size = batch_size * num_steps\n",
    "    n_batches = int(len(chars) / slice_size)\n",
    "    \n",
    "    # Drop the last few characters to make only full batches\n",
    "    x = chars[: n_batches*slice_size]\n",
    "    y = chars[1: n_batches*slice_size + 1]\n",
    "    \n",
    "    # Split the data into batch_size slices, then stack them into a 2D matrix \n",
    "    x = np.stack(np.split(x, batch_size))\n",
    "    y = np.stack(np.split(y, batch_size))\n",
    "    \n",
    "    # Now x and y are arrays with dimensions batch_size x n_batches*num_steps\n",
    "    \n",
    "    # Split into training and validation sets, keep the first split_frac batches for training\n",
    "    split_idx = int(n_batches*split_frac)\n",
    "    train_x, train_y= x[:, :split_idx*num_steps], y[:, :split_idx*num_steps]\n",
    "    val_x, val_y = x[:, split_idx*num_steps:], y[:, split_idx*num_steps:]\n",
    "    \n",
    "    return train_x, train_y, val_x, val_y\n",
    "```\n",
    "* **Get Batches**\n",
    "\n",
    "```python\n",
    "def get_batch(arrs, num_steps):\n",
    "    batch_size, slice_size = arrs[0].shape\n",
    "    \n",
    "    n_batches = int(slice_size/num_steps)\n",
    "    for b in range(n_batches):\n",
    "        yield [x[:, b*num_steps: (b+1)*num_steps] for x in arrs]\n",
    "```\n",
    "* **LSTM**\n",
    "\n",
    "```python\n",
    "# Use a basic LSTM cell\n",
    "lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "    \n",
    "# Add dropout to the cell\n",
    "drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "    \n",
    "# Stack up multiple LSTM layers, for deep learning\n",
    "cell = tf.contrib.rnn.MultiRNNCell([drop] * num_layers)\n",
    "initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "### Run the data through the RNN layers\n",
    "# Run each sequence step through the RNN and collect the outputs\n",
    "outputs, state = tf.nn.dynamic_rnn(cell, x_one_hot, initial_state=initial_state)\n",
    "final_state = state\n",
    "```\n",
    "## Path\n",
    "> deep-learning/intro-to-rnns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Skip-gram word2vec\n",
    "## Goal\n",
    "> implement the word2vec algorithm using the skip-gram architecture\n",
    "\n",
    "## Main Libs\n",
    "> numpy, tensorflow\n",
    "\n",
    "## Technology & Core Code\n",
    "* **Subsampling —— Reduce the noise**\n",
    "\n",
    "```python\n",
    "threshold = 1e-5\n",
    "word_counts = Counter(int_words)\n",
    "total_count = len(int_words)\n",
    "freqs = {word: count/total_count for word, count in word_counts.items()}\n",
    "p_drop = {word: 1 - np.sqrt(threshold/freqs[word]) for word in word_counts}\n",
    "train_words = [word for word in int_words if p_drop[word] < random.random()]\n",
    "```\n",
    "* **Making batches**\n",
    "\n",
    "```python\n",
    "def get_target(words, idx, window_size=5):\n",
    "    ''' Get a list of words in a window around an index. '''\n",
    "    \n",
    "    # Your code here\n",
    "    R = np.random.randint(1, window_size+1)\n",
    "    start = idx - R if (idx - R) > 0 else 0\n",
    "    stop = idx + R\n",
    "    target_words = set(words[start:idx] + words[idx+1:stop+1])\n",
    "    \n",
    "    return list(target_words)\n",
    "   \n",
    "def get_batches(words, batch_size, window_size=5):\n",
    "    ''' Create a generator of word batches as a tuple (inputs, targets) '''\n",
    "    \n",
    "    n_batches = len(words)//batch_size\n",
    "    \n",
    "    # only full batches\n",
    "    words = words[:n_batches*batch_size]\n",
    "    \n",
    "    for idx in range(0, len(words), batch_size):\n",
    "        x, y = [], []\n",
    "        batch = words[idx:idx+batch_size]\n",
    "        for ii in range(len(batch)):\n",
    "            batch_x = batch[ii]\n",
    "            batch_y = get_target(batch, ii, window_size)\n",
    "            y.extend(batch_y)\n",
    "            x.extend([batch_x]*len(batch_y))\n",
    "        yield x, ys\n",
    "```\n",
    "* **Embedding**\n",
    "\n",
    "```python\n",
    "n_vocab = len(int_to_vocab)\n",
    "n_embedding =  200 # Number of embedding features \n",
    "with train_graph.as_default():\n",
    "    embedding = tf.Variable(tf.random_uniform((n_vocab, n_embedding), -1, 1))\n",
    "    embed = tf.nn.embedding_lookup(embedding, inputs)\n",
    "```\n",
    "* **Negative Sampling**\n",
    "```python\n",
    "# Number of negative labels to sample\n",
    "n_sampled = 100\n",
    "with train_graph.as_default():\n",
    "    softmax_w = tf.Variable(tf.truncated_normal((n_vocab, n_embedding), stddev=0.1))\n",
    "    softmax_b = tf.Variable(tf.zeros(n_vocab))\n",
    "    \n",
    "    # Calculate the loss using negative sampling\n",
    "    loss = tf.nn.sampled_softmax_loss(softmax_w, softmax_b, \n",
    "                                      labels, embed,\n",
    "                                      n_sampled, n_vocab)\n",
    "    \n",
    "    cost = tf.reduce_mean(loss)\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "```\n",
    "## Path\n",
    "> deep-learning/embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 8. Weight Initialization\n",
    "## Goal\n",
    "> Find good initial weights for a neural network\n",
    "\n",
    "## Main Libs\n",
    "> tensorflow, numpy\n",
    "\n",
    "## Technology & Core Code\n",
    "* **Uniform Distribution**\n",
    "\n",
    "```python\n",
    "uniform_neg1to1_weights = [\n",
    "    tf.Variable(tf.random_uniform(layer_1_weight_shape, -1, 1)),\n",
    "    tf.Variable(tf.random_uniform(layer_2_weight_shape, -1, 1)),\n",
    "    tf.Variable(tf.random_uniform(layer_3_weight_shape, -1, 1))\n",
    "]\n",
    "```\n",
    "* **Normal Distribution**\n",
    "\n",
    "```python\n",
    "normal_01_weights = [\n",
    "    tf.Variable(tf.random_normal(layer_1_weight_shape, stddev=0.1)),\n",
    "    tf.Variable(tf.random_normal(layer_2_weight_shape, stddev=0.1)),\n",
    "    tf.Variable(tf.random_normal(layer_3_weight_shape, stddev=0.1))\n",
    "]\n",
    "```\n",
    "* **Truncated Normal Distribution**\n",
    "```python\n",
    "trunc_normal_01_weights = [\n",
    "    tf.Variable(tf.truncated_normal(layer_1_weight_shape, stddev=0.1)),\n",
    "    tf.Variable(tf.truncated_normal(layer_2_weight_shape, stddev=0.1)),\n",
    "    tf.Variable(tf.truncated_normal(layer_3_weight_shape, stddev=0.1))\n",
    "]\n",
    "```\n",
    "## Path\n",
    "> deep-learning/weight-initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Sentiment Analysis with an RNN\n",
    "## Goal\n",
    ">  Implementing a recurrent neural network that performs sentiment analysis. \n",
    "\n",
    "## Main Libs\n",
    "> tensorflow, numpy, Counter\n",
    "\n",
    "## Technology & Core Code\n",
    "* **Data preprocessing**\n",
    "\n",
    "```python\n",
    "from string import punctuation\n",
    "all_text = ''.join([c for c in reviews if c not in punctuation])\n",
    "reviews = all_text.split('\\n')\n",
    "\n",
    "all_text = ' '.join(reviews)\n",
    "words = all_text.split()\n",
    "```\n",
    "* **Encoding the words**\n",
    "\n",
    "```python\n",
    "# Create your dictionary that maps vocab words to integers here\n",
    "from collections import Counter\n",
    "counts = Counter(words)\n",
    "vocab = sorted(counts, key=counts.get, reverse=True)\n",
    "vocab_to_int = {word: ii for ii, word in enumerate(vocab, 1)}\n",
    "\n",
    "# Convert the reviews to integers, same shape as reviews list, but with integers\n",
    "reviews_ints = []\n",
    "for each in reviews:\n",
    "    reviews_ints.append([vocab_to_int[word] for word in each.split()])\n",
    "```\n",
    "* **Embedding the labels**\n",
    "\n",
    "```python\n",
    "# Convert labels to 1s and 0s for 'positive' and 'negative'\n",
    "labels = labels.split('\\n')\n",
    "labels = np.array([1 if each == 'positive' else 0 for each in labels])\n",
    "```\n",
    "* **Embedding**\n",
    "\n",
    "```python\n",
    "# Size of the embedding vectors (number of units in the embedding layer)\n",
    "embed_size = 300 \n",
    "\n",
    "with graph.as_default():\n",
    "    embedding = tf.Variable(tf.random_uniform((n_words, embed_size), -1, 1))\n",
    "    embed = tf.nn.embedding_lookup(embedding, inputs_)\n",
    "```\n",
    "* **LSTM cell**\n",
    "\n",
    "```python\n",
    "with graph.as_default():\n",
    "    # Your basic LSTM cell\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size) \n",
    "    \n",
    "    # Add dropout to the cell\n",
    "    drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "    \n",
    "    # Stack up multiple LSTM layers, for deep learning\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([drop] * lstm_layers)\n",
    "    \n",
    "    # Getting an initial state of all zeros\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "```\n",
    "* **RNN forward pass**\n",
    "\n",
    "```python\n",
    "with graph.as_default():\n",
    "    outputs, final_state = tf.nn.dynamic_rnn(cell, embed,\n",
    "                                             initial_state=initial_state)\n",
    "```\n",
    "## Path\n",
    "> deep-learning/sentiment-rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 10. TV Script Generation\n",
    "## Goal\n",
    "> Generating your own Simpsons TV scripts using RNNs\n",
    "\n",
    "## Main Libs\n",
    "> numpy, tensorflow, collection\n",
    "\n",
    "## Technology & Core Code\n",
    "* **Lookup Table**\n",
    "\n",
    "```python\n",
    "def create_lookup_tables(text):\n",
    "    \"\"\"\n",
    "    Create lookup tables for vocabulary\n",
    "    :param text: The text of tv scripts split into words\n",
    "    :return: A tuple of dicts (vocab_to_int, int_to_vocab)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    counts = Counter(text)\n",
    "    vocab = sorted(counts, key=counts.get, reverse=True)\n",
    "    vocab_to_int = {word: idx for idx, word in enumerate(vocab)}\n",
    "    int_to_vocab = {idx: word for idx, word in enumerate(vocab)}\n",
    "    \n",
    "    return vocab_to_int, int_to_vocab\n",
    "```\n",
    "* **Tokenize Punctuation**\n",
    "\n",
    "```python\n",
    "def token_lookup():\n",
    "    \"\"\"\n",
    "    Generate a dict to turn punctuation into a token.\n",
    "    :return: Tokenize dictionary where the key is the punctuation and the value is the token\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    punc_to_token = {'.':'Period',\n",
    "                     ',':'Comma',\n",
    "                     '\"':'Quotation_Mark',\n",
    "                     ';':'Semicolon',\n",
    "                     '!':'Exclamation_mark',\n",
    "                     '?':'Question_mark',\n",
    "                     '(':'Left_Parentheses',\n",
    "                     ')':'Right_Parentheses',\n",
    "                     '--':'Dash',\n",
    "                     '\\n':'Return'};\n",
    "    return punc_to_token\n",
    "```\n",
    "* **Build RNN Cell and Initialize**\n",
    "\n",
    "```python\n",
    "def get_init_cell(batch_size, rnn_size):\n",
    "    \"\"\"\n",
    "    Create an RNN Cell and initialize it.\n",
    "    :param batch_size: Size of batches\n",
    "    :param rnn_size: Size of RNNs\n",
    "    :return: Tuple (cell, initialize state)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    num_layers = 1\n",
    "    keep_prob =0.5\n",
    "    \n",
    "    rnn = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "    # drop = tf.contrib.rnn.DropoutWrapper(rnn, output_keep_prob=keep_prob)\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([rnn] * num_layers)\n",
    "    \n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    initial_state = tf.identity(initial_state, name=\"initial_state\")\n",
    "    \n",
    "    return cell, initial_state\n",
    "```\n",
    "* **Word Embedding**\n",
    "\n",
    "```python\n",
    "def get_embed(input_data, vocab_size, embed_dim):\n",
    "    \"\"\"\n",
    "    Create embedding for <input_data>.\n",
    "    :param input_data: TF placeholder for text input.\n",
    "    :param vocab_size: Number of words in vocabulary.\n",
    "    :param embed_dim: Number of embedding dimensions\n",
    "    :return: Embedded input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1))\n",
    "    embed = tf.nn.embedding_lookup(embedding, input_data)\n",
    "    return embed\n",
    "```\n",
    "* **Build RNN**\n",
    "\n",
    "```python\n",
    "def build_rnn(cell, inputs):\n",
    "    \"\"\"\n",
    "    Create a RNN using a RNN Cell\n",
    "    :param cell: RNN Cell\n",
    "    :param inputs: Input text data\n",
    "    :return: Tuple (Outputs, Final State)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    outputs, final_state = tf.nn.dynamic_rnn(cell, inputs,dtype=tf.float32)\n",
    "    final_state = tf.identity(final_state, \"final_state\")\n",
    "    return outputs, final_state\n",
    "```\n",
    "* **Build the Neural Network**\n",
    "\n",
    "```python\n",
    "def build_nn(cell, rnn_size, input_data, vocab_size, embed_dim):\n",
    "    \"\"\"\n",
    "    Build part of the neural network\n",
    "    :param cell: RNN cell\n",
    "    :param rnn_size: Size of rnns\n",
    "    :param input_data: Input data\n",
    "    :param vocab_size: Vocabulary size\n",
    "    :param embed_dim: Number of embedding dimensions\n",
    "    :return: Tuple (Logits, FinalState)\n",
    "    \"\"\"\n",
    "    input_data = get_embed(input_data, vocab_size, embed_dim)\n",
    "    outputs, final_state = build_rnn(cell, input_data)\n",
    "    logits = tf.contrib.layers.fully_connected(outputs, vocab_size, activation_fn = None)\n",
    "    \n",
    "    # TODO: Implement Function\n",
    "    return logits, final_state\n",
    "\n",
    "```\n",
    "* **Batches**\n",
    "\n",
    "```python\n",
    "def get_batches(int_text, batch_size, seq_length):\n",
    "    \"\"\"\n",
    "    Return batches of input and target\n",
    "    :param int_text: Text with the words replaced by their ids\n",
    "    :param batch_size: The size of batch\n",
    "    :param seq_length: The length of sequence\n",
    "    :return: Batches as a Numpy array\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    n_batches = len(int_text)//(batch_size*seq_length)\n",
    "    x, y = np.array(int_text[:n_batches*batch_size*seq_length]), np.array(int_text[1:n_batches*batch_size*seq_length+1])\n",
    "    x_batches = np.split(x.reshape(batch_size, -1), n_batches, 1)\n",
    "    y_batches = np.split(y.reshape(batch_size, -1), n_batches, 1)\n",
    "    return np.array(list(zip(x_batches, y_batches)))         \n",
    "```\n",
    "\n",
    "\n",
    "## Path\n",
    "> deep-learning/tv-script-generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 11. Transfer Learning\n",
    "## Goal\n",
    "> Apply transfer learning to classify flowers by using VGGNet\n",
    "\n",
    "## Main Libs\n",
    "> tensorflow, np, sklearn\n",
    "\n",
    "## Technology & Core Code\n",
    "* **Spliting Data**\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "ss = StratifiedShuffleSplit(n_splits=1, test_size=0.2)\n",
    "\n",
    "train_idx, val_idx = next(ss.split(codes, labels))\n",
    "\n",
    "half_val_len = int(len(val_idx)/2)\n",
    "val_idx, test_idx = val_idx[:half_val_len], val_idx[half_val_len:]\n",
    "\n",
    "train_x, train_y = codes[train_idx], labels_vecs[train_idx]\n",
    "val_x, val_y = codes[val_idx], labels_vecs[val_idx]\n",
    "test_x, test_y =  codes[test_idx], labels_vecs[test_idx]\n",
    "```\n",
    "* **Classifier layers**\n",
    "\n",
    "```python\n",
    "inputs_ = tf.placeholder(tf.float32, shape=[None, codes.shape[1]])\n",
    "labels_ = tf.placeholder(tf.int64, shape=[None, labels_vecs.shape[1]])\n",
    "\n",
    "# TODO: Classifier layers and operations\n",
    "fc = tf.contrib.layers.fully_connected(inputs_, 256)\n",
    "\n",
    "logits = tf.contrib.layers.fully_connected(fc, labels_vecs.shape[1], activation_fn=None) # output layer logits\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=labels_, logits=logits)\n",
    "cost = tf.reduce_mean(cross_entropy) # cross entropy loss\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost) # training optimizer\n",
    "\n",
    "# Operations for validation/test accuracy\n",
    "predicted = tf.nn.softmax(logits)\n",
    "correct_pred = tf.equal(tf.argmax(predicted, 1), tf.argmax(labels_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "```\n",
    "## Path\n",
    "> deep-learning/transfer-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 12. Character Sequence to Sequence\n",
    "## Goal\n",
    "> Building a model that takes in a sequence of letters, and outputs a sorted version of that sequence\n",
    "\n",
    "## Main Libs\n",
    "> tensorflow, np, sklearn\n",
    "\n",
    "## Technology & Core Code\n",
    "* **Preprocess**\n",
    "\n",
    "To do anything useful with it, we'll need to turn the characters into a list of integers:\n",
    "\n",
    "```python\n",
    "def extract_character_vocab(data):\n",
    "    special_words = ['<pad>', '<unk>', '<s>',  '<\\s>']\n",
    "\n",
    "    set_words = set([character for line in data.split('\\n') for character in line])\n",
    "    int_to_vocab = {word_i: word for word_i, word in enumerate(special_words + list(set_words))}\n",
    "    vocab_to_int = {word: word_i for word_i, word in int_to_vocab.items()}\n",
    "\n",
    "    return int_to_vocab, vocab_to_int\n",
    "\n",
    "# Build int2letter and letter2int dicts\n",
    "source_int_to_letter, source_letter_to_int = extract_character_vocab(source_sentences)\n",
    "target_int_to_letter, target_letter_to_int = extract_character_vocab(target_sentences)\n",
    "\n",
    "# Convert characters to ids\n",
    "source_letter_ids = [[source_letter_to_int.get(letter, source_letter_to_int['<unk>']) for letter in line] for line in source_sentences.split('\\n')]\n",
    "target_letter_ids = [[target_letter_to_int.get(letter, target_letter_to_int['<unk>']) for letter in line] for line in target_sentences.split('\\n')]\n",
    "```\n",
    "Determine the the longest sequence size in the dataset we'll be using, then pad all the sequences to that length.\n",
    "```python\n",
    "def pad_id_sequences(source_ids, source_letter_to_int, target_ids, target_letter_to_int, sequence_length):\n",
    "    new_source_ids = [sentence + [source_letter_to_int['<pad>']] * (sequence_length - len(sentence)) \\\n",
    "                      for sentence in source_ids]\n",
    "    new_target_ids = [sentence + [target_letter_to_int['<pad>']] * (sequence_length - len(sentence)) \\\n",
    "                      for sentence in target_ids]\n",
    "\n",
    "    return new_source_ids, new_target_ids\n",
    "\n",
    "\n",
    "# Use the longest sequence as sequence length\n",
    "sequence_length = max(\n",
    "        [len(sentence) for sentence in source_letter_ids] + [len(sentence) for sentence in target_letter_ids])\n",
    "\n",
    "# Pad all sequences up to sequence length\n",
    "source_ids, target_ids = pad_id_sequences(source_letter_ids, source_letter_to_int, \n",
    "                                          target_letter_ids, target_letter_to_int, sequence_length)\n",
    "```\n",
    "* **Encoding**\n",
    "\n",
    "Embed the input data using [`tf.contrib.layers.embed_sequence`](https://www.tensorflow.org/api_docs/python/tf/contrib/layers/embed_sequence)\n",
    "\n",
    "Pass the embedded input into a stack of RNNs.  Save the RNN state and ignore the output.\n",
    "```python\n",
    "source_vocab_size = len(source_letter_to_int)\n",
    "\n",
    "# Encoder embedding\n",
    "enc_embed_input = tf.contrib.layers.embed_sequence(input_data, source_vocab_size, encoding_embedding_size)\n",
    "\n",
    "# Encoder\n",
    "enc_cell = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.BasicLSTMCell(rnn_size)] * num_layers)\n",
    "_, enc_state = tf.nn.dynamic_rnn(enc_cell, enc_embed_input, dtype=tf.float32)\n",
    "```\n",
    "* **Process Decoding Input**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Process the input we'll feed to the decoder\n",
    "ending = tf.strided_slice(targets, [0, 0], [batch_size, -1], [1, 1])\n",
    "dec_input = tf.concat([tf.fill([batch_size, 1], target_letter_to_int['<s>']), ending], 1)\n",
    "\n",
    "demonstration_outputs = np.reshape(range(batch_size * sequence_length), (batch_size, sequence_length))\n",
    "```\n",
    "* **Decoding**\n",
    "\n",
    "```python\n",
    "target_vocab_size = len(target_letter_to_int)\n",
    "\n",
    "# Decoder Embedding\n",
    "dec_embeddings = tf.Variable(tf.random_uniform([target_vocab_size, decoding_embedding_size]))\n",
    "dec_embed_input = tf.nn.embedding_lookup(dec_embeddings, dec_input)\n",
    "\n",
    "# Decoder RNNs\n",
    "dec_cell = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.BasicLSTMCell(rnn_size)] * num_layers)\n",
    "\n",
    "with tf.variable_scope(\"decoding\") as decoding_scope:\n",
    "    # Output Layer\n",
    "    output_fn = lambda x: tf.contrib.layers.fully_connected(x, target_vocab_size, None, scope=decoding_scope)\n",
    "```\n",
    "* **Decoder During Training**\n",
    "\n",
    "```python\n",
    "with tf.variable_scope(\"decoding\") as decoding_scope:\n",
    "    # Training Decoder\n",
    "    train_decoder_fn = tf.contrib.seq2seq.simple_decoder_fn_train(enc_state)\n",
    "    train_pred, _, _ = tf.contrib.seq2seq.dynamic_rnn_decoder(\n",
    "        dec_cell, train_decoder_fn, dec_embed_input, sequence_length, scope=decoding_scope)\n",
    "    \n",
    "    # Apply output function\n",
    "    train_logits =  output_fn(train_pred)\n",
    "```\n",
    "* **Decoder During Inference**\n",
    "```python\n",
    "with tf.variable_scope(\"decoding\", reuse=True) as decoding_scope:\n",
    "    # Inference Decoder\n",
    "    infer_decoder_fn = tf.contrib.seq2seq.simple_decoder_fn_inference(\n",
    "        output_fn, enc_state, dec_embeddings, target_letter_to_int['<s>'], target_letter_to_int['<\\s>'], \n",
    "        sequence_length - 1, target_vocab_size)\n",
    "    inference_logits, _, _ = tf.contrib.seq2seq.dynamic_rnn_decoder(dec_cell, infer_decoder_fn, scope=decoding_scope)\n",
    "```\n",
    "* **Optimization**\n",
    "\n",
    "```python\n",
    "# Loss function\n",
    "cost = tf.contrib.seq2seq.sequence_loss(\n",
    "    train_logits,\n",
    "    targets,\n",
    "    tf.ones([batch_size, sequence_length]))\n",
    "\n",
    "# Optimizer\n",
    "optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "# Gradient Clipping\n",
    "gradients = optimizer.compute_gradients(cost)\n",
    "capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n",
    "train_op = optimizer.apply_gradients(capped_gradients)\n",
    "```\n",
    "\n",
    "## Path\n",
    "> deep-learning/seq2seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13. Deep Q-learning\n",
    "## Goal\n",
    "> Build a neural network that can learn to play games through reinforcement learning\n",
    "\n",
    "## Main Libs\n",
    "> tensorflow, np, collections\n",
    "\n",
    "## Technology & Core Code\n",
    "* **Q-Network**\n",
    "\n",
    "```python\n",
    "class QNetwork:\n",
    "    def __init__(self, learning_rate=0.01, state_size=4, \n",
    "                 action_size=2, hidden_size=10, \n",
    "                 name='QNetwork'):\n",
    "        # state inputs to the Q-network\n",
    "        with tf.variable_scope(name):\n",
    "            self.inputs_ = tf.placeholder(tf.float32, [None, state_size], name='inputs')\n",
    "            \n",
    "            # One hot encode the actions to later choose the Q-value for the action\n",
    "            self.actions_ = tf.placeholder(tf.int32, [None], name='actions')\n",
    "            one_hot_actions = tf.one_hot(self.actions_, action_size)\n",
    "            \n",
    "            # Target Q values for training\n",
    "            self.targetQs_ = tf.placeholder(tf.float32, [None], name='target')\n",
    "            \n",
    "            # ReLU hidden layers\n",
    "            self.fc1 = tf.contrib.layers.fully_connected(self.inputs_, hidden_size)\n",
    "            self.fc2 = tf.contrib.layers.fully_connected(self.fc1, hidden_size)\n",
    "\n",
    "            # Linear output layer\n",
    "            self.output = tf.contrib.layers.fully_connected(self.fc2, action_size, \n",
    "                                                            activation_fn=None)\n",
    "            \n",
    "            ### Train with loss (targetQ - Q)^2\n",
    "            # output has length 2, for two actions. This lext line chooses\n",
    "            # one value from output (per row) according to the one-hot encoded actions.\n",
    "            self.Q = tf.reduce_sum(tf.multiply(self.output, one_hot_actions), axis=1)\n",
    "            \n",
    "            self.loss = tf.reduce_mean(tf.square(self.targetQs_ - self.Q))\n",
    "            self.opt = tf.train.AdamOptimizer(learning_rate).minimize(self.loss)\n",
    "```\n",
    "* **Experience replay**\n",
    "\n",
    "```python\n",
    "class Memory():\n",
    "    def __init__(self, max_size = 1000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "    \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "            \n",
    "    def sample(self, batch_size):\n",
    "        idx = np.random.choice(np.arange(len(self.buffer)), \n",
    "                               size=batch_size, \n",
    "                               replace=False)\n",
    "        return [self.buffer[ii] for ii in idx]\n",
    "```\n",
    "* **Populate the experience memory**\n",
    "\n",
    "```python\n",
    "# Initialize the simulation\n",
    "env.reset()\n",
    "# Take one random step to get the pole and cart moving\n",
    "state, reward, done, _ = env.step(env.action_space.sample())\n",
    "\n",
    "memory = Memory(max_size=memory_size)\n",
    "\n",
    "# Make a bunch of random actions and store the experiences\n",
    "for ii in range(pretrain_length):\n",
    "    # Uncomment the line below to watch the simulation\n",
    "    # env.render()\n",
    "\n",
    "    # Make a random action\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "    if done:\n",
    "        # The simulation fails so no next state\n",
    "        next_state = np.zeros(state.shape)\n",
    "        # Add experience to memory\n",
    "        memory.add((state, action, reward, next_state))\n",
    "        \n",
    "        # Start new episode\n",
    "        env.reset()\n",
    "        # Take one random step to get the pole and cart moving\n",
    "        state, reward, done, _ = env.step(env.action_space.sample())\n",
    "    else:\n",
    "        # Add experience to memory\n",
    "        memory.add((state, action, reward, next_state))\n",
    "        state = next_state\n",
    "```\n",
    "* **Training**\n",
    "\n",
    "```python\n",
    "# Now train with experiences\n",
    "saver = tf.train.Saver()\n",
    "rewards_list = []\n",
    "with tf.Session() as sess:\n",
    "    # Initialize variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    step = 0\n",
    "    for ep in range(1, train_episodes):\n",
    "        total_reward = 0\n",
    "        t = 0\n",
    "        while t < max_steps:\n",
    "            step += 1\n",
    "            # Uncomment this next line to watch the training\n",
    "            # env.render() \n",
    "            \n",
    "            # Explore or Exploit\n",
    "            explore_p = explore_stop + (explore_start - explore_stop)*np.exp(-decay_rate*step) \n",
    "            if explore_p > np.random.rand():\n",
    "                # Make a random action\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                # Get action from Q-network\n",
    "                feed = {mainQN.inputs_: state.reshape((1, *state.shape))}\n",
    "                Qs = sess.run(mainQN.output, feed_dict=feed)\n",
    "                action = np.argmax(Qs)\n",
    "            \n",
    "            # Take action, get new state and reward\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "    \n",
    "            total_reward += reward\n",
    "            \n",
    "            if done:\n",
    "                # the episode ends so no next state\n",
    "                next_state = np.zeros(state.shape)\n",
    "                t = max_steps\n",
    "                \n",
    "                print('Episode: {}'.format(ep),\n",
    "                      'Total reward: {}'.format(total_reward),\n",
    "                      'Training loss: {:.4f}'.format(loss),\n",
    "                      'Explore P: {:.4f}'.format(explore_p))\n",
    "                rewards_list.append((ep, total_reward))\n",
    "                \n",
    "                # Add experience to memory\n",
    "                memory.add((state, action, reward, next_state))\n",
    "                \n",
    "                # Start new episode\n",
    "                env.reset()\n",
    "                # Take one random step to get the pole and cart moving\n",
    "                state, reward, done, _ = env.step(env.action_space.sample())\n",
    "\n",
    "            else:\n",
    "                # Add experience to memory\n",
    "                memory.add((state, action, reward, next_state))\n",
    "                state = next_state\n",
    "                t += 1\n",
    "            \n",
    "            # Sample mini-batch from memory\n",
    "            batch = memory.sample(batch_size)\n",
    "            states = np.array([each[0] for each in batch])\n",
    "            actions = np.array([each[1] for each in batch])\n",
    "            rewards = np.array([each[2] for each in batch])\n",
    "            next_states = np.array([each[3] for each in batch])\n",
    "            \n",
    "            # Train network\n",
    "            target_Qs = sess.run(mainQN.output, feed_dict={mainQN.inputs_: next_states})\n",
    "            \n",
    "            episode_ends = (next_states == np.zeros(states[0].shape)).all(axis=1)\n",
    "            target_Qs[episode_ends] = (0, 0)\n",
    "            \n",
    "            targets = rewards + gamma * np.max(target_Qs, axis=1)\n",
    "\n",
    "            loss, _ = sess.run([mainQN.loss, mainQN.opt],\n",
    "                                feed_dict={mainQN.inputs_: states,\n",
    "                                           mainQN.targetQs_: targets,\n",
    "                                           mainQN.actions_: actions})\n",
    "        \n",
    "    saver.save(sess, \"checkpoints/cartpole.ckpt\")\n",
    "\n",
    "```\n",
    "* **Visualizing training**\n",
    "\n",
    "```python\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / N \n",
    "    \n",
    "eps, rews = np.array(rewards_list).T\n",
    "smoothed_rews = running_mean(rews, 10)\n",
    "plt.plot(eps[-len(smoothed_rews):], smoothed_rews)\n",
    "plt.plot(eps, rews, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "```\n",
    "\n",
    "## Path\n",
    "> deep-learning/reinforcement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14. A Simple Autoencoder\n",
    "## Goal\n",
    "> Building a simple autoencoder to compress the MNIST dataset.\n",
    "\n",
    "## Main Libs\n",
    "> tensorflow, np, matplotlib\n",
    "\n",
    "## Technology & Core Code\n",
    "* **Build the graph for the autoencoder in the cell below.**\n",
    "\n",
    "```python\n",
    "# Size of the encoding layer (the hidden layer)\n",
    "encoding_dim = 32\n",
    "\n",
    "image_size = mnist.train.images.shape[1]\n",
    "\n",
    "inputs_ = tf.placeholder(tf.float32, (None, image_size), name='inputs')\n",
    "targets_ = tf.placeholder(tf.float32, (None, image_size), name='targets')\n",
    "\n",
    "# Output of hidden layer\n",
    "encoded = tf.layers.dense(inputs_, encoding_dim, activation=tf.nn.relu)\n",
    "\n",
    "# Output layer logits\n",
    "logits = tf.layers.dense(encoded, image_size, activation=None)\n",
    "# Sigmoid output from\n",
    "decoded = tf.nn.sigmoid(logits, name='output')\n",
    "\n",
    "loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=targets_, logits=logits)\n",
    "cost = tf.reduce_mean(loss)\n",
    "opt = tf.train.AdamOptimizer(0.001).minimize(cost)\n",
    "```\n",
    "* **Training**\n",
    "```python\n",
    "epochs = 20\n",
    "batch_size = 200\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for e in range(epochs):\n",
    "    for ii in range(mnist.train.num_examples//batch_size):\n",
    "        batch = mnist.train.next_batch(batch_size)\n",
    "        feed = {inputs_: batch[0], targets_: batch[0]}\n",
    "        batch_cost, _ = sess.run([cost, opt], feed_dict=feed)\n",
    "\n",
    "        print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "              \"Training loss: {:.4f}\".format(batch_cost))\n",
    "```\n",
    "\n",
    "## Path\n",
    "> deep-learning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15. Convolutional Autoencoder\n",
    "## Goal\n",
    "> Sticking with the MNIST dataset, let's improve our autoencoder's performance using convolutional layers.\n",
    "\n",
    "## Main Libs\n",
    "> tensorflow, np, sklearn\n",
    "\n",
    "## Technology & Core Code\n",
    "* **What's going on with the decoder**\n",
    "\n",
    "```python\n",
    "inputs_ = tf.placeholder(tf.float32, (None, 28, 28, 1), name='inputs')\n",
    "targets_ = tf.placeholder(tf.float32, (None, 28, 28, 1), name='targets')\n",
    "\n",
    "### Encoder\n",
    "conv1 = tf.layers.conv2d(inputs_, 16, (3,3), padding='same', activation=tf.nn.relu)\n",
    "# Now 28x28x16\n",
    "maxpool1 = tf.layers.max_pooling2d(conv1, (2,2), (2,2), padding='same')\n",
    "# Now 14x14x16\n",
    "conv2 = tf.layers.conv2d(maxpool1, 8, (3,3), padding='same', activation=tf.nn.relu)\n",
    "# Now 14x14x8\n",
    "maxpool2 = tf.layers.max_pooling2d(conv2, (2,2), (2,2), padding='same')\n",
    "# Now 7x7x8\n",
    "conv3 = tf.layers.conv2d(maxpool2, 8, (3,3), padding='same', activation=tf.nn.relu)\n",
    "# Now 7x7x8\n",
    "encoded = tf.layers.max_pooling2d(conv3, (2,2), (2,2), padding='same')\n",
    "# Now 4x4x8\n",
    "\n",
    "### Decoder\n",
    "upsample1 = tf.image.resize_nearest_neighbor(encoded, (7,7))\n",
    "# Now 7x7x8\n",
    "conv4 = tf.layers.conv2d(upsample1, 8, (3,3), padding='same', activation=tf.nn.relu)\n",
    "# Now 7x7x8\n",
    "upsample2 = tf.image.resize_nearest_neighbor(conv4, (14,14))\n",
    "# Now 14x14x8\n",
    "conv5 = tf.layers.conv2d(upsample2, 8, (3,3), padding='same', activation=tf.nn.relu)\n",
    "# Now 14x14x8\n",
    "upsample3 = tf.image.resize_nearest_neighbor(conv5, (28,28))\n",
    "# Now 28x28x8\n",
    "conv6 = tf.layers.conv2d(upsample3, 16, (3,3), padding='same', activation=tf.nn.relu)\n",
    "# Now 28x28x16\n",
    "\n",
    "logits = tf.layers.conv2d(conv6, 1, (3,3), padding='same', activation=None)\n",
    "#Now 28x28x1\n",
    "\n",
    "decoded = tf.nn.sigmoid(logits, name='decoded')\n",
    "\n",
    "loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=targets_, logits=logits)\n",
    "cost = tf.reduce_mean(loss)\n",
    "opt = tf.train.AdamOptimizer(0.001).minimize(cost)\n",
    "```\n",
    "* **Denoising**\n",
    "\n",
    "```python\n",
    "inputs_ = tf.placeholder(tf.float32, (None, 28, 28, 1), name='inputs')\n",
    "targets_ = tf.placeholder(tf.float32, (None, 28, 28, 1), name='targets')\n",
    "\n",
    "### Encoder\n",
    "conv1 = tf.layers.conv2d(inputs_, 32, (3,3), padding='same', activation=tf.nn.relu)\n",
    "# Now 28x28x32\n",
    "maxpool1 = tf.layers.max_pooling2d(conv1, (2,2), (2,2), padding='same')\n",
    "# Now 14x14x32\n",
    "conv2 = tf.layers.conv2d(maxpool1, 32, (3,3), padding='same', activation=tf.nn.relu)\n",
    "# Now 14x14x32\n",
    "maxpool2 = tf.layers.max_pooling2d(conv2, (2,2), (2,2), padding='same')\n",
    "# Now 7x7x32\n",
    "conv3 = tf.layers.conv2d(maxpool2, 16, (3,3), padding='same', activation=tf.nn.relu)\n",
    "# Now 7x7x16\n",
    "encoded = tf.layers.max_pooling2d(conv3, (2,2), (2,2), padding='same')\n",
    "# Now 4x4x16\n",
    "\n",
    "### Decoder\n",
    "upsample1 = tf.image.resize_nearest_neighbor(encoded, (7,7))\n",
    "# Now 7x7x16\n",
    "conv4 = tf.layers.conv2d(upsample1, 16, (3,3), padding='same', activation=tf.nn.relu)\n",
    "# Now 7x7x16\n",
    "upsample2 = tf.image.resize_nearest_neighbor(conv4, (14,14))\n",
    "# Now 14x14x16\n",
    "conv5 = tf.layers.conv2d(upsample2, 32, (3,3), padding='same', activation=tf.nn.relu)\n",
    "# Now 14x14x32\n",
    "upsample3 = tf.image.resize_nearest_neighbor(conv5, (28,28))\n",
    "# Now 28x28x32\n",
    "conv6 = tf.layers.conv2d(upsample3, 32, (3,3), padding='same', activation=tf.nn.relu)\n",
    "# Now 28x28x32\n",
    "\n",
    "logits = tf.layers.conv2d(conv6, 1, (3,3), padding='same', activation=None)\n",
    "#Now 28x28x1\n",
    "\n",
    "decoded = tf.nn.sigmoid(logits, name='decoded')\n",
    "\n",
    "loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=targets_, logits=logits)\n",
    "cost = tf.reduce_mean(loss)\n",
    "opt = tf.train.AdamOptimizer(0.001).minimize(cost)\n",
    "```\n",
    "## Path\n",
    "> deep-learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Transfer Learning\n",
    "## Goal\n",
    "> Apply transfer learning to classify flowers by using VGGNet\n",
    "\n",
    "## Main Libs\n",
    "> tensorflow, np, sklearn\n",
    "\n",
    "## Technology & Core Code\n",
    "> \n",
    "\n",
    "## Path\n",
    "> deep-learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

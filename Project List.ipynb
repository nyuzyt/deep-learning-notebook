{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 1. Your first neural network\n",
    "## Goal\n",
    "> Build a 3-layer neural network with basic libs\n",
    "\n",
    "## Main Libs\n",
    "> numpy, pandas, matplotlib.pyplot\n",
    "\n",
    "## Technology\n",
    "> Weight Initialization\n",
    "\n",
    "> Matrix Calculation\n",
    "\n",
    "> Backpropagation\n",
    "\n",
    "## Core Code\n",
    "```python\n",
    "class NeuralNetwork(object):\n",
    "    def __init__(self, input_nodes, hidden_nodes, output_nodes, learning_rate):\n",
    "        # Set number of nodes in input, hidden and output layers.\n",
    "        self.input_nodes = input_nodes\n",
    "        self.hidden_nodes = hidden_nodes\n",
    "        self.output_nodes = output_nodes\n",
    "\n",
    "        # Initialize weights\n",
    "        self.weights_input_to_hidden = np.random.normal(0.0, self.hidden_nodes**-0.5, \n",
    "                                       (self.hidden_nodes, self.input_nodes))\n",
    "\n",
    "        self.weights_hidden_to_output = np.random.normal(0.0, self.output_nodes**-0.5, \n",
    "                                       (self.output_nodes, self.hidden_nodes))\n",
    "        self.lr = learning_rate\n",
    "        \n",
    "        #### Set this to your implemented sigmoid function ####\n",
    "        # Activation function is the sigmoid function\n",
    "    def activation_function(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def train(self, inputs_list, targets_list):\n",
    "        # Convert inputs list to 2d array\n",
    "        inputs = np.array(inputs_list, ndmin=2).T\n",
    "        targets = np.array(targets_list, ndmin=2).T\n",
    "        \n",
    "        #### Implement the forward pass here ####\n",
    "        ### Forward pass ###\n",
    "        # TODO: Hidden layer\n",
    "        hidden_inputs = np.dot(self.weights_input_to_hidden, inputs)\n",
    "        hidden_outputs = self.activation_function(hidden_inputs)\n",
    "        \n",
    "        # TODO: Output layer\n",
    "        final_inputs = np.dot(self.weights_hidden_to_output, hidden_outputs)\n",
    "        final_outputs = final_inputs\n",
    "        \n",
    "        #### Implement the backward pass here ####\n",
    "        ### Backward pass ###\n",
    "        \n",
    "        # TODO: Output error\n",
    "        output_errors = targets - final_outputs\n",
    "        \n",
    "        # TODO: Backpropagated error\n",
    "        hidden_errors = np.dot(self.weights_hidden_to_output.T, output_errors)\n",
    "        hidden_grad = hidden_outputs * (1.0 - hidden_outputs)\n",
    "\n",
    "        # TODO: Update the weights\n",
    "        self.weights_hidden_to_output += self.lr * np.dot(output_errors, hidden_outputs.T)\n",
    "        self.weights_input_to_hidden += self.lr * np.dot(hidden_errors * hidden_grad, inputs.T)\n",
    "    \n",
    "    def run(self, inputs_list):\n",
    "        # Run a forward pass through the network\n",
    "        inputs = np.array(inputs_list, ndmin=2).T\n",
    "        \n",
    "        #### Implement the forward pass here ####\n",
    "        # TODO: Hidden layer\n",
    "        hidden_inputs = np.dot(self.weights_input_to_hidden, inputs)\n",
    "        hidden_outputs = self.activation_function(hidden_inputs)\n",
    "        \n",
    "        # TODO: Output layer\n",
    "        final_inputs = np.dot(self.weights_hidden_to_output,hidden_outputs)\n",
    "        final_outputs = final_inputs\n",
    "        \n",
    "        return final_outputs\n",
    "```\n",
    "## Path\n",
    "> deep-learning/first-neural-network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Sentiment Classification\n",
    "## Goal\n",
    "> Build a neural network which can tell positive and negative reviews\n",
    "\n",
    "## Main Libs\n",
    "> collections, numpy\n",
    "\n",
    "## Technology\n",
    "> Count Word\n",
    "\n",
    "> Reduce Noise\n",
    "\n",
    "> Reduce Inefficiencies\n",
    "\n",
    "## Core Code\n",
    "```python\n",
    "def pre_process_data(self, reviews, labels, polarity_cutoff, min_count):\n",
    "        \n",
    "        ## ----------------------------------------\n",
    "        ## New for Project 6: Calculate positive-to-negative ratios for words before\n",
    "        #                     building vocabulary\n",
    "        #\n",
    "        positive_counts = Counter()\n",
    "        negative_counts = Counter()\n",
    "        total_counts = Counter()\n",
    "\n",
    "        for i in range(len(reviews)):\n",
    "            if(labels[i] == 'POSITIVE'):\n",
    "                for word in reviews[i].split(\" \"):\n",
    "                    positive_counts[word] += 1\n",
    "                    total_counts[word] += 1\n",
    "            else:\n",
    "                for word in reviews[i].split(\" \"):\n",
    "                    negative_counts[word] += 1\n",
    "                    total_counts[word] += 1\n",
    "\n",
    "        pos_neg_ratios = Counter()\n",
    "\n",
    "        for term,cnt in list(total_counts.most_common()):\n",
    "            if(cnt >= 50):\n",
    "                pos_neg_ratio = positive_counts[term] / float(negative_counts[term]+1)\n",
    "                pos_neg_ratios[term] = pos_neg_ratio\n",
    "\n",
    "        for word,ratio in pos_neg_ratios.most_common():\n",
    "            if(ratio > 1):\n",
    "                pos_neg_ratios[word] = np.log(ratio)\n",
    "            else:\n",
    "                pos_neg_ratios[word] = -np.log((1 / (ratio + 0.01)))\n",
    "        #\n",
    "        ## end New for Project 6\n",
    "        ## ----------------------------------------\n",
    "\n",
    "        # populate review_vocab with all of the words in the given reviews\n",
    "        review_vocab = set()\n",
    "        for review in reviews:\n",
    "            for word in review.split(\" \"):\n",
    "                ## New for Project 6: only add words that occur at least min_count times\n",
    "                #                     and for words with pos/neg ratios, only add words\n",
    "                #                     that meet the polarity_cutoff\n",
    "                if(total_counts[word] > min_count):\n",
    "                    if(word in pos_neg_ratios.keys()):\n",
    "                        if((pos_neg_ratios[word] >= polarity_cutoff) or (pos_neg_ratios[word] <= -polarity_cutoff)):\n",
    "                            review_vocab.add(word)\n",
    "                    else:\n",
    "                        review_vocab.add(word)\n",
    "\n",
    "        # Convert the vocabulary set to a list so we can access words via indices\n",
    "        self.review_vocab = list(review_vocab)\n",
    "        \n",
    "        # populate label_vocab with all of the words in the given labels.\n",
    "        label_vocab = set()\n",
    "        for label in labels:\n",
    "            label_vocab.add(label)\n",
    "        \n",
    "        # Convert the label vocabulary set to a list so we can access labels via indices\n",
    "        self.label_vocab = list(label_vocab)\n",
    "        \n",
    "        # Store the sizes of the review and label vocabularies.\n",
    "        self.review_vocab_size = len(self.review_vocab)\n",
    "        self.label_vocab_size = len(self.label_vocab)\n",
    "        \n",
    "        # Create a dictionary of words in the vocabulary mapped to index positions\n",
    "        self.word2index = {}\n",
    "        for i, word in enumerate(self.review_vocab):\n",
    "            self.word2index[word] = i\n",
    "        \n",
    "        # Create a dictionary of labels mapped to index positions\n",
    "        self.label2index = {}\n",
    "        for i, label in enumerate(self.label_vocab):\n",
    "            self.label2index[label] = i\n",
    "```\n",
    "## Path\n",
    "> deep-learning/sentiment-network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Sentiment Analysis with TFLearn\n",
    "## Goal\n",
    "> Build a neural network for sentiment analysis using a library called TFLearn\n",
    "\n",
    "## Main Libs\n",
    "> pandas, numpy, tensorflow, tflearn\n",
    "\n",
    "## Technology\n",
    "> Count Word\n",
    "\n",
    "> TFLearn functions\n",
    "\n",
    "## Core Code\n",
    "```python\n",
    "def build_model():\n",
    "    # This resets all parameters and variables, leave this here\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    #### Your code ####\n",
    "    net = tflearn.input_data([None, 10000])                          # Input\n",
    "    net = tflearn.fully_connected(net, 200, activation='ReLU')      # Hidden\n",
    "    net = tflearn.fully_connected(net, 25, activation='ReLU')      # Hidden\n",
    "    \n",
    "    net = tflearn.fully_connected(net, 2, activation='softmax')   # Output\n",
    "    net = tflearn.regression(net, optimizer='sgd', learning_rate=0.01, loss='categorical_crossentropy')\n",
    "    \n",
    "    model = tflearn.DNN(net)\n",
    "    return model\n",
    "```\n",
    "\n",
    "## Path\n",
    "> deep-learning/intro-to-tflearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. TensorFlow Neural Network Lab\n",
    "## Goal\n",
    "> Labeling images of English letters with tensorflow\n",
    "\n",
    "## Main Libs\n",
    "> tensorflow, sklearn\n",
    "\n",
    "## Technology\n",
    "> normalization\n",
    "\n",
    "> One-Hot Encode\n",
    "\n",
    "> split data\n",
    "\n",
    "> tensorflow operation\n",
    "\n",
    "## Core Code\n",
    "```python\n",
    "# Change if you have memory restrictions\n",
    "batch_size = 128\n",
    "\n",
    "# TODO: Find the best parameters for each configuration\n",
    "epochs = 5\n",
    "learning_rate = 0.1\n",
    "\n",
    "\n",
    "### DON'T MODIFY ANYTHING BELOW ###\n",
    "# Gradient Descent\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)    \n",
    "\n",
    "# The accuracy measured against the validation set\n",
    "validation_accuracy = 0.0\n",
    "\n",
    "# Measurements use for graphing loss and accuracy\n",
    "log_batch_step = 50\n",
    "batches = []\n",
    "loss_batch = []\n",
    "train_acc_batch = []\n",
    "valid_acc_batch = []\n",
    "\n",
    "with tf.Session() as session:\n",
    "    session.run(init)\n",
    "    batch_count = int(math.ceil(len(train_features)/batch_size))\n",
    "\n",
    "    for epoch_i in range(epochs):\n",
    "        \n",
    "        # Progress bar\n",
    "        batches_pbar = tqdm(range(batch_count), desc='Epoch {:>2}/{}'.format(epoch_i+1, epochs), unit='batches')\n",
    "        \n",
    "        # The training cycle\n",
    "        for batch_i in batches_pbar:\n",
    "            # Get a batch of training features and labels\n",
    "            batch_start = batch_i*batch_size\n",
    "            batch_features = train_features[batch_start:batch_start + batch_size]\n",
    "            batch_labels = train_labels[batch_start:batch_start + batch_size]\n",
    "\n",
    "            # Run optimizer and get loss\n",
    "            _, l = session.run(\n",
    "                [optimizer, loss],\n",
    "                feed_dict={features: batch_features, labels: batch_labels})\n",
    "\n",
    "            # Log every 50 batches\n",
    "            if not batch_i % log_batch_step:\n",
    "                # Calculate Training and Validation accuracy\n",
    "                training_accuracy = session.run(accuracy, feed_dict=train_feed_dict)\n",
    "                validation_accuracy = session.run(accuracy, feed_dict=valid_feed_dict)\n",
    "\n",
    "                # Log batches\n",
    "                previous_batch = batches[-1] if batches else 0\n",
    "                batches.append(log_batch_step + previous_batch)\n",
    "                loss_batch.append(l)\n",
    "                train_acc_batch.append(training_accuracy)\n",
    "                valid_acc_batch.append(validation_accuracy)\n",
    "\n",
    "        # Check accuracy against Validation data\n",
    "        validation_accuracy = session.run(accuracy, feed_dict=valid_feed_dict)\n",
    "```\n",
    "\n",
    "## Path\n",
    "> deep-learning/intro-to-tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Image Classification\n",
    "## Goal\n",
    "> Train a convolutional neural network to classify images\n",
    "\n",
    "## Main Libs\n",
    "> tensorflow, numpy\n",
    "\n",
    "## Technology\n",
    "> normalization\n",
    "\n",
    "> One-Hot encode\n",
    "\n",
    "> Convolution and Max Pooling Layer\n",
    "\n",
    "> Fully-Connected Layer\n",
    "\n",
    "## Core Code\n",
    "```python\n",
    "  def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    conv_num_outputs = 32\n",
    "    conv_ksize = (4,4) \n",
    "    conv_strides = (2,2) \n",
    "    pool_ksize = (4,4)\n",
    "    pool_strides = (2,2)\n",
    "    conv = conv2d_maxpool(x, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    conv = tf.nn.dropout(conv, keep_prob)\n",
    "    \n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    flat = flatten(conv)\n",
    "\n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    fullyConn1 = fully_conn(flat, 128)\n",
    "    fullyConn1 = tf.nn.dropout(fullyConn1, keep_prob)\n",
    "    fullyConn2 = fully_conn(fullyConn1, 32)\n",
    "    fullyConn2 = tf.nn.dropout(fullyConn2, keep_prob)\n",
    "    \n",
    "    # TODO: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    out = output(fullyConn2, 10)\n",
    "    \n",
    "    # TODO: return output\n",
    "    return out\n",
    "```\n",
    "\n",
    "## Path\n",
    "> deep-learning/image-classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Anna KaRNNa\n",
    "## Goal\n",
    "> Building a character-wise RNN trained on Anna Karenina, it'll be able to generate new text based on the text from the book\n",
    "\n",
    "## Main Libs\n",
    "> collection, numpy, tensorflow\n",
    "\n",
    "## Technology\n",
    "> split data\n",
    "\n",
    "> LSTM\n",
    "\n",
    "> gradient clipping\n",
    "\n",
    "## Core Code\n",
    "```python\n",
    "def build_rnn(num_classes, batch_size=50, num_steps=50, lstm_size=128, num_layers=2,\n",
    "              learning_rate=0.001, grad_clip=5, sampling=False):\n",
    "    \n",
    "    # When we're using this network for sampling later, we'll be passing in\n",
    "    # one character at a time, so providing an option for that\n",
    "    if sampling == True:\n",
    "        batch_size, num_steps = 1, 1\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # Declare placeholders we'll feed into the graph\n",
    "    inputs = tf.placeholder(tf.int32, [batch_size, num_steps], name='inputs')\n",
    "    targets = tf.placeholder(tf.int32, [batch_size, num_steps], name='targets')\n",
    "    \n",
    "    # Keep probability placeholder for drop out layers\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    # One-hot encoding the input and target characters\n",
    "    x_one_hot = tf.one_hot(inputs, num_classes)\n",
    "    y_one_hot = tf.one_hot(targets, num_classes)\n",
    "\n",
    "    ### Build the RNN layers\n",
    "    # Use a basic LSTM cell\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "    \n",
    "    # Add dropout to the cell\n",
    "    drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "    \n",
    "    # Stack up multiple LSTM layers, for deep learning\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([drop] * num_layers)\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "    ### Run the data through the RNN layers\n",
    "    # Run each sequence step through the RNN and collect the outputs\n",
    "    outputs, state = tf.nn.dynamic_rnn(cell, x_one_hot, initial_state=initial_state)\n",
    "    final_state = state\n",
    "    \n",
    "    # Reshape output so it's a bunch of rows, one output row for each step for each batch\n",
    "    seq_output = tf.concat(outputs, axis=1)\n",
    "    output = tf.reshape(seq_output, [-1, lstm_size])\n",
    "    \n",
    "    # Now connect the RNN outputs to a softmax layer\n",
    "    with tf.variable_scope('softmax'):\n",
    "        softmax_w = tf.Variable(tf.truncated_normal((lstm_size, num_classes), stddev=0.1))\n",
    "        softmax_b = tf.Variable(tf.zeros(num_classes))\n",
    "    \n",
    "    # Since output is a bunch of rows of RNN cell outputs, logits will be a bunch\n",
    "    # of rows of logit outputs, one for each step and batch\n",
    "    logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "    \n",
    "    # Use softmax to get the probabilities for predicted characters\n",
    "    preds = tf.nn.softmax(logits, name='predictions')\n",
    "    \n",
    "    # Reshape the targets to match the logits\n",
    "    y_reshaped = tf.reshape(y_one_hot, [-1, num_classes])\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_reshaped)\n",
    "    cost = tf.reduce_mean(loss)\n",
    "\n",
    "    # Optimizer for training, using gradient clipping to control exploding gradients\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), grad_clip)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "    optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "    \n",
    "    # Export the nodes\n",
    "    # NOTE: I'm using a namedtuple here because I think they are cool\n",
    "    export_nodes = ['inputs', 'targets', 'initial_state', 'final_state',\n",
    "                    'keep_prob', 'cost', 'preds', 'optimizer']\n",
    "    Graph = namedtuple('Graph', export_nodes)\n",
    "    local_dict = locals()\n",
    "    graph = Graph(*[local_dict[each] for each in export_nodes])\n",
    "    \n",
    "    return graph\n",
    "```\n",
    "\n",
    "## Path\n",
    "> deep-learning/intro-to-rnns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Skip-gram word2vec\n",
    "## Goal\n",
    "> implement the word2vec algorithm using the skip-gram architecture\n",
    "\n",
    "## Main Libs\n",
    "> numpy, tensorflow\n",
    "\n",
    "## Technology\n",
    "> Subsampling —— Reduce the noise\n",
    "\n",
    "> Embedding\n",
    "\n",
    "> Making batches\n",
    "\n",
    "## Core Code\n",
    "```python\n",
    "def get_target(words, idx, window_size=5):\n",
    "    ''' Get a list of words in a window around an index. '''\n",
    "    \n",
    "    # Your code here\n",
    "    R = np.random.randint(1, window_size+1)\n",
    "    start = idx - R if (idx - R) > 0 else 0\n",
    "    stop = idx + R\n",
    "    target_words = set(words[start:idx] + words[idx+1:stop+1])\n",
    "    \n",
    "    return list(target_words)\n",
    "   \n",
    "def get_batches(words, batch_size, window_size=5):\n",
    "    ''' Create a generator of word batches as a tuple (inputs, targets) '''\n",
    "    \n",
    "    n_batches = len(words)//batch_size\n",
    "    \n",
    "    # only full batches\n",
    "    words = words[:n_batches*batch_size]\n",
    "    \n",
    "    for idx in range(0, len(words), batch_size):\n",
    "        x, y = [], []\n",
    "        batch = words[idx:idx+batch_size]\n",
    "        for ii in range(len(batch)):\n",
    "            batch_x = batch[ii]\n",
    "            batch_y = get_target(batch, ii, window_size)\n",
    "            y.extend(batch_y)\n",
    "            x.extend([batch_x]*len(batch_y))\n",
    "        yield x, ys\n",
    "```\n",
    "\n",
    "## Path\n",
    "> deep-learning/embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 8. Weight Initialization\n",
    "## Goal\n",
    "> Find good initial weights for a neural network\n",
    "\n",
    "## Main Libs\n",
    "> tensorflow, numpy\n",
    "\n",
    "## Technology\n",
    "> Uniform Distribution\n",
    "\n",
    "> Normal Distribution\n",
    "\n",
    "## Core Code\n",
    "```python\n",
    "uniform_neg1to1_weights = [\n",
    "    tf.Variable(tf.random_uniform(layer_1_weight_shape, -1, 1)),\n",
    "    tf.Variable(tf.random_uniform(layer_2_weight_shape, -1, 1)),\n",
    "    tf.Variable(tf.random_uniform(layer_3_weight_shape, -1, 1))\n",
    "]\n",
    "\n",
    "normal_01_weights = [\n",
    "    tf.Variable(tf.random_normal(layer_1_weight_shape, stddev=0.1)),\n",
    "    tf.Variable(tf.random_normal(layer_2_weight_shape, stddev=0.1)),\n",
    "    tf.Variable(tf.random_normal(layer_3_weight_shape, stddev=0.1))\n",
    "]\n",
    "\n",
    "trunc_normal_01_weights = [\n",
    "    tf.Variable(tf.truncated_normal(layer_1_weight_shape, stddev=0.1)),\n",
    "    tf.Variable(tf.truncated_normal(layer_2_weight_shape, stddev=0.1)),\n",
    "    tf.Variable(tf.truncated_normal(layer_3_weight_shape, stddev=0.1))\n",
    "]\n",
    "```\n",
    "\n",
    "## Path\n",
    "> deep-learning/weight-initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Sentiment Analysis with an RNN\n",
    "## Goal\n",
    ">  Implementing a recurrent neural network that performs sentiment analysis. \n",
    "\n",
    "## Main Libs\n",
    "> tensorflow, numpy, Counter\n",
    "\n",
    "## Technology\n",
    "> Data preprocessing\n",
    "\n",
    "> Encoding the words\n",
    "\n",
    "> Embedding\n",
    "\n",
    "> LSTM cell\n",
    "\n",
    "> RNN forward pass\n",
    "\n",
    "## Core Code\n",
    "```python\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    iteration = 1\n",
    "    for e in range(epochs):\n",
    "        state = sess.run(initial_state)\n",
    "        \n",
    "        for ii, (x, y) in enumerate(get_batches(train_x, train_y, batch_size), 1):\n",
    "            feed = {inputs_: x,\n",
    "                    labels_: y[:, None],\n",
    "                    keep_prob: 0.5,\n",
    "                    initial_state: state}\n",
    "            loss, state, _ = sess.run([cost, final_state, optimizer], feed_dict=feed)\n",
    "            \n",
    "            if iteration%5==0:\n",
    "                print(\"Epoch: {}/{}\".format(e, epochs),\n",
    "                      \"Iteration: {}\".format(iteration),\n",
    "                      \"Train loss: {:.3f}\".format(loss))\n",
    "\n",
    "            if iteration%25==0:\n",
    "                val_acc = []\n",
    "                val_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "                for x, y in get_batches(val_x, val_y, batch_size):\n",
    "                    feed = {inputs_: x,\n",
    "                            labels_: y[:, None],\n",
    "                            keep_prob: 1,\n",
    "                            initial_state: val_state}\n",
    "                    batch_acc, val_state = sess.run([accuracy, final_state], feed_dict=feed)\n",
    "                    val_acc.append(batch_acc)\n",
    "                print(\"Val acc: {:.3f}\".format(np.mean(val_acc)))\n",
    "            iteration +=1\n",
    "    saver.save(sess, \"checkpoints/sentiment.ckpt\")\n",
    "```\n",
    "\n",
    "## Path\n",
    "> deep-learning/sentiment-rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 10. TV Script Generation\n",
    "## Goal\n",
    "> Generating your own Simpsons TV scripts using RNNs\n",
    "\n",
    "## Main Libs\n",
    "> numpy, tensorflow, collection\n",
    "\n",
    "## Technology\n",
    "> Lookup Table\n",
    "\n",
    "> Tokenize Punctuation\n",
    "\n",
    "> Build RNN Cell and Initialize\n",
    "\n",
    "> Batches\n",
    "\n",
    "## Core Code\n",
    "```python\n",
    "embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1))\n",
    "    embed = tf.nn.embedding_lookup(embedding, input_data)\n",
    "    \n",
    "def get_batches(int_text, batch_size, seq_length):\n",
    "    \"\"\"\n",
    "    Return batches of input and target\n",
    "    :param int_text: Text with the words replaced by their ids\n",
    "    :param batch_size: The size of batch\n",
    "    :param seq_length: The length of sequence\n",
    "    :return: Batches as a Numpy array\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    n_batches = len(int_text)//(batch_size*seq_length)\n",
    "    x, y = np.array(int_text[:n_batches*batch_size*seq_length]), np.array(int_text[1:n_batches*batch_size*seq_length+1])\n",
    "    x_batches = np.split(x.reshape(batch_size, -1), n_batches, 1)\n",
    "    y_batches = np.split(y.reshape(batch_size, -1), n_batches, 1)\n",
    "    return np.array(list(zip(x_batches, y_batches)))  \n",
    "```\n",
    "\n",
    "## Path\n",
    "> deep-learning/tv-script-generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NaN. Project Name\n",
    "## Goal\n",
    "> \n",
    "\n",
    "## Main Libs\n",
    "> \n",
    "\n",
    "## Technology\n",
    "> \n",
    "\n",
    "## Core Code\n",
    "```python\n",
    "```\n",
    "\n",
    "## Path\n",
    "> deep-learning\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
